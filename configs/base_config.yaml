multiprocessing: False                              # Run the models and samples in parallel
path_pretrained_models: './pretrained_models'       # Path to the pretrained models

load_models:                                        # Which pretrained models to load
    maskrcnn: False
    clip: False
    glip: True
    owlvit: False
    tcl: False
    gpt3_qa: False
    gpt3_general: False
    depth: True
    blip: True
    saliency: False
    xvlm: True
    codex: True

models_gpu:                                        # Which GPU for each pretrained models
    maskrcnn: 1
    clip: 1
    glip: 3
    owlvit: 1
    tcl: 1
    gpt3_qa: 1
    gpt3_general: 1
    depth: 3
    blip: 4
    saliency: 1
    xvlm: 4
    codex: 1

detect_thresholds:                                  # Thresholds for the models that perform detection
    glip: 0.5
    maskrcnn: 0.8
    owlvit: 0.1
ratio_box_area_to_image_area: 0.0                   # Any detected patch under this size will not be returned
crop_larger_margin: True                            # Increase size of crop by 10% to include more context

verify_property:                                    # Parameters for verify_property
    model: xvlm                                     # Model to use for verify_property
    thresh_clip: 0.6
    thresh_tcl: 0.25
    thresh_xvlm: 0.6

best_match_model: xvlm                              # Which model to use for best_[image, text]_match

gpt3:                                               # GPT-3 configuration
    n_votes: 1                                      # Number of tries to use for GPT-3. Use with temperature > 0
    qa_prompt: ./prompts/gpt3/gpt3_qa.txt
    temperature: 0.                                 # Temperature for GPT-3. Almost deterministic if 0
    model: text-davinci-003                         # Can replace with code-davinci-002 (which is free for now) but will have worse performance as it's meant for code

codex:
    temperature: 0.                                 # Temperature for Codex. (Almost) deterministic if 0
    best_of: 1                                      # Number of tries to choose from. Use when temperature > 0
    max_tokens: 512                                 # Maximum number of tokens to generate for Codex
    prompt: ./prompts/api.prompt                    # Codex prompt file, which defines the API. If you use a Chat-based model (3.5/4) try ./prompts/chatapi.prompt (doesn't support video for now due to token limits)
    model: gpt-3.5-turbo                        # Codex model to use. [code-davinci-002, gpt-3.5-turbo, gpt-4]


blip_half_precision: True                           # Use 8bit (Faster but slightly less accurate) for BLIP if True
blip_v2_model_type: blip2-flan-t5-xl               # Which model to use for BLIP-2
